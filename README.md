# Word Embeddings
*Word embedding* is used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using a set of language modeling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers. This project is mainly inspired from the research work of *Mikolov et al.* published in the paper <a href="https://arxiv.org/pdf/1301.3781.pdf"> *Efficient Estimation of Word Representations in Vector Space* </a>. In the paper, the authors have proposed two novel model architectures for computing continuous vector representations of words from very large data sets, *Continuous Bag-of-Words Model* and *Continuous Skip-gram Model*. This project uses the *Continuous Skip-gram Model* to learn word embeddings from an *11 million* words text corpus, having a vocabulary size of *202,000* (without re-sampling and filtering). In conjunction with the basics discussed in this paper, the project also implements the advanced sampling techniques and extensions of the Continuous Skip-gram Model presented in another paper of *Mikolov et al.*, <a href="https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf"> *Distributed Representations of Words and Phrases and their Compositionality* </a>. These extensions improve both the quality of the vectors and the training speed. By subsampling of the frequent words, significant speedup is obtained and also more regular word representations are learned.
