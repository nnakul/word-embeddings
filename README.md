# word-embeddings
*Word embedding* is used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using a set of language modeling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers. This project is mainly inspired from the research work of *Mikolov et al.* published in the paper <a href="https://arxiv.org/pdf/1301.3781.pdf"> *Efficient Estimation of Word Representations in Vector Space* </a>. In the paper, the authors have proposed two novel model architectures for computing continuous vector representations of words from very large data sets, *Continuous Bag-of-Words Model* and *Continuous Skip-gram Model*. This project uses the *Continuous Skip-gram Model* to learn word embeddings from an *11 million* words text corpus with a vocabulary size of *202,000* (without re-sampling and filtering)
